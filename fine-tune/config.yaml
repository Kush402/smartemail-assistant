model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  max_length: 512
  padding: "max_length"
  truncation: true
  use_4bit: true

training:
  output_dir: "model/checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 50
  fp16: true
  max_length: 512

lora:
  r: 16
  alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
