model:
  base_model: "gpt2-medium"
  max_length: 512
  padding: "max_length"
  truncation: true

training:
  output_dir: "model/checkpoints"
  num_train_epochs: 6
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "loss"
  greater_is_better: false

lora:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: ["q_proj", "v_proj"]
