{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# SmartEmail Assistant - Google Colab Setup with Llama 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook helps you set up and run the SmartEmail Assistant project on Google Colab with GPU acceleration using Llama 2.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Important Setup Steps:\\n\",\n",
    "    \"1. Make sure to select a GPU runtime (Runtime > Change runtime type > GPU)\\n\",\n",
    "    \"2. You'll need a Hugging Face account with access to Llama 2\\n\",\n",
    "    \"3. Set up your Hugging Face token in the notebook\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Check GPU Availability\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"!nvidia-smi\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Install Required Libraries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"!pip install -q pandas numpy scikit-learn transformers torch peft datasets tqdm pyyaml wandb python-dotenv accelerate bitsandbytes sentencepiece protobuf huggingface_hub einops\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Set up Hugging Face Authentication\\n\",\n",
    "    \"\\n\",\n",
    "    \"You need to:\\n\",\n",
    "    \"1. Create a Hugging Face account at https://huggingface.co/\\n\",\n",
    "    \"2. Request access to Llama 2 at https://huggingface.co/meta-llama/Llama-2-7b-hf\\n\",\n",
    "    \"3. Create an access token at https://huggingface.co/settings/tokens\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from huggingface_hub import login\\n\",\n",
    "    \"login()  # This will prompt for your token\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Clone the Repository\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"!git clone https://github.com/Kush402/smartemail-assistant.git\\n\",\n",
    "    \"%cd smartemail-assistant\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Upload Training Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Upload your `raw_emails.csv` file to the `data/raw` directory. You can do this by:\\n\",\n",
    "    \"1. Click on the folder icon in the left sidebar\\n\",\n",
    "    \"2. Navigate to `data/raw`\\n\",\n",
    "    \"3. Click the upload button and select your file\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Create necessary directories\\n\",\n",
    "    \"!mkdir -p data/raw data/processed model/checkpoints model/peft_adapter\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Run the Training Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now that everything is set up, you can run the training pipeline. The notebook will:\\n\",\n",
    "    \"1. Load and preprocess your data\\n\",\n",
    "    \"2. Set up the Llama 2 model with LoRA\\n\",\n",
    "    \"3. Train the model\\n\",\n",
    "    \"4. Save the trained model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import the training pipeline\\n\",\n",
    "    \"from notebooks.smartemail_pipeline import *\\n\",\n",
    "    \"\\n\",\n",
    "    \"# The notebook will automatically use GPU if available\\n\",\n",
    "    \"print(\\\"CUDA available:\\\", torch.cuda.is_available())\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    print(\\\"GPU count:\\\", torch.cuda.device_count())\\n\",\n",
    "    \"    print(\\\"GPU name:\\\", torch.cuda.get_device_name(0))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Download the Trained Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"After training is complete, you can download the model files:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from google.colab import files\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a zip file of the model directory\\n\",\n",
    "    \"!zip -r model.zip model/\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download the zip file\\n\",\n",
    "    \"files.download('model.zip')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Additional Notes\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Make sure to select a GPU runtime in Colab (Runtime > Change runtime type > GPU)\\n\",\n",
    "    \"2. The training process might take 2-3 hours depending on the dataset size\\n\",\n",
    "    \"3. You can monitor the training progress through the output and Weights & Biases dashboard\\n\",\n",
    "    \"4. The model checkpoints will be saved in the `model/checkpoints` directory\\n\",\n",
    "    \"5. The LoRA adapter will be saved in the `model/peft_adapter` directory\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Troubleshooting\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you encounter any issues:\\n\",\n",
    "    \"1. Make sure you're using a GPU runtime\\n\",\n",
    "    \"2. Check if all dependencies are installed correctly\\n\",\n",
    "    \"3. Verify that your training data is in the correct format\\n\",\n",
    "    \"4. Ensure you have enough disk space in Colab\\n\",\n",
    "    \"5. If you get import errors, try restarting the runtime after installing packages\\n\",\n",
    "    \"6. If you get CUDA out of memory errors, try reducing the batch size or using gradient accumulation\\n\",\n",
    "    \"7. Check the Colab logs for any specific error messages\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  },\n",
    "  \"accelerator\": \"GPU\"\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
